{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371b2240-7f76-40d2-9243-ffdd254948c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# HÃœCRE -1: KÃœTÃœPHANE GÃœNCELLEME (HER ÅEYDEN Ã–NCE BUNU Ã‡ALIÅTIR)\n",
    "# ============================================================================\n",
    "print(\"TÃ¼m gerekli kÃ¼tÃ¼phaneler en gÃ¼ncel versiyonlara yÃ¼kseltiliyor...\")\n",
    "print(\"Bu iÅŸlem birkaÃ§ dakika sÃ¼rebilir...\")\n",
    "\n",
    "# '!' iÅŸareti, bu komutun bir Python kodu deÄŸil, bir terminal komutu olduÄŸunu belirtir.\n",
    "!pip install --upgrade --quiet google-generativeai langchain-google-genai langchain langchain-chroma langchain-huggingface\n",
    "\n",
    "print(\"\\nâœ… GÃ¼ncelleme tamamlandÄ±! LÃœTFEN BÄ°R SONRAKÄ° ADIMA GEÃ‡Ä°N.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b416c6-d26e-40b5-993b-4634f9c8c75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# HÃœCRE 0: MODEL TEÅHÄ°S ARACI\n",
    "# Bu kod, API anahtarÄ±nÄ±zla hangi Gemini modellerini kullanabileceÄŸinizi listeler.\n",
    "# ============================================================================\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "print(\"API AnahtarÄ±nÄ±z iÃ§in kullanÄ±labilir Gemini modelleri listeleniyor...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    api_key = os.environ.get('GOOGLE_API_KEY')\n",
    "    if not api_key:\n",
    "        raise ValueError(\"API AnahtarÄ± bulunamadÄ±! LÃ¼tfen 'GOOGLE_API_KEY' ortam deÄŸiÅŸkenini kontrol edin.\")\n",
    "    \n",
    "    genai.configure(api_key=api_key)\n",
    "\n",
    "    # API'den, 'generateContent' metodunu destekleyen tÃ¼m modelleri listelemesini isteyelim.\n",
    "    model_found = False\n",
    "    for m in genai.list_models():\n",
    "        if 'generateContent' in m.supported_generation_methods:\n",
    "            print(f\"âœ… Model AdÄ±: {m.name}\")\n",
    "            print(f\"   AÃ§Ä±klama: {m.description}\\n\")\n",
    "            model_found = True\n",
    "            \n",
    "    if not model_found:\n",
    "        print(\"âŒ 'generateContent' metodunu destekleyen hiÃ§bir model bulunamadÄ±.\")\n",
    "        print(\"   LÃ¼tfen Google AI Studio veya Google Cloud Console Ã¼zerinden projenizi ve API anahtarÄ±nÄ±zÄ± kontrol edin.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ğŸš¨ Modeller listelenirken bir hata oluÅŸtu: {e}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TeÅŸhis tamamlandÄ±. LÃ¼tfen yukarÄ±daki 'Model AdÄ±' listesinden birini kopyalayÄ±n.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab1899c-e059-474e-a7df-7864e61c589f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# DENEME 1: Google Gemini API ile Veri Ãœretme (Ä°lk Hata)\n",
    "\n",
    "# HEDEF: Google Gemini API'sinin Ã¼cretsiz katmanÄ±nÄ± kullanarak cevap varyasyonlarÄ± Ã¼retmek.\n",
    "#\n",
    "# KARÅILAÅILAN SORUN: GÃ¼nlÃ¼k Kota HatasÄ± (429 ResourceExhausted)\n",
    "#   - Hata MesajÄ±: \"Quota exceeded for metric: ...generate_content_free_tier_requests, limit: 200\"\n",
    "#   - TEÅHÄ°S: Script bir sÃ¼re (yaklaÅŸÄ±k 90-100 soru) Ã§alÄ±ÅŸtÄ±ktan sonra duruyordu.\n",
    "#     Bunun sebebinin, Google'Ä±n Ã¼cretsiz API katmanÄ±na koyduÄŸu \"GÃ¼nlÃ¼k Toplam Ä°stek Limiti\"\n",
    "#     olduÄŸu anlaÅŸÄ±ldÄ±. Dakikalarca beklemek bu sorunu Ã§Ã¶zmÃ¼yordu Ã§Ã¼nkÃ¼ limit gÃ¼nlÃ¼k olarak sÄ±fÄ±rlanÄ±yordu.\n",
    "#\n",
    "#   - Ã‡Ã–ZÃœM PLANI: FarklÄ± APÄ° KEY'Ler alarak proje iÃ§erisinde 6 farklÄ± APÄ° KEY KullanÄ±ldÄ±.\n",
    "\n",
    "\n",
    "# DENEME 2: Hugging Face API'ye GeÃ§iÅŸ (KÃ¼tÃ¼phane ve Model HatalarÄ±)\n",
    "\n",
    "# HEDEF: Google'Ä±n gÃ¼nlÃ¼k limitini aÅŸmak iÃ§in Hugging Face'in Ã¼cretsiz Inference API'Ä±nÄ± kullanmak.\n",
    "#        Model olarak Meta'nÄ±n gÃ¼Ã§lÃ¼ Llama-3 modeli seÃ§ildi.\n",
    "#\n",
    "# KARÅILAÅILAN SORUN 1: KÃ¼tÃ¼phane Uyumluluk HatasÄ± ('thinking' HatasÄ±)\n",
    "#   - Hata MesajÄ±: \"Model.__init__() got an unexpected keyword argument 'thinking'\"\n",
    "#   - TEÅHÄ°S: Bu hatanÄ±n temel sebebi, `google-generativeai` ve `langchain` gibi\n",
    "#     kÃ¼tÃ¼phanelerin bilgisayarÄ±mÄ±zda yÃ¼klÃ¼ olan versiyonlarÄ±nÄ±n eski olmasÄ±ydÄ±.\n",
    "#     API'dan gelen yeni bir alanÄ± tanÄ±yamÄ±yorlardÄ±.\n",
    "#   - Ã‡Ã–ZÃœM: Projenin en baÅŸÄ±na bir kurulum hÃ¼cresi eklenerek tÃ¼m kÃ¼tÃ¼phaneler\n",
    "#     `!pip install --upgrade` komutuyla en gÃ¼ncel sÃ¼rÃ¼mlerine yÃ¼kseltildi.\n",
    "#\n",
    "# KARÅILAÅILAN SORUN 2: Model GÃ¶rev UyumsuzluÄŸu HatasÄ±\n",
    "#   - Hata MesajÄ±: \"Model ... is not supported for task text-generation... Supported task: conversational\"\n",
    "#   - TEÅHÄ°S: LangChain'deki `HuggingFaceEndpoint` aracÄ±nÄ±n, Llama-3-Instruct gibi\n",
    "#     modelleri \"sohbet\" formatÄ± yerine \"dÃ¼z metin Ã¼retme\" formatÄ±nda Ã§aÄŸÄ±rmaya Ã§alÄ±ÅŸtÄ±ÄŸÄ± anlaÅŸÄ±ldÄ±.\n",
    "#\n",
    "# KARÅILAÅILAN SORUN 3: YanlÄ±ÅŸ SarmalayÄ±cÄ± (Wrapper) KullanÄ±mÄ±\n",
    "#   - Hata MesajÄ±: \"ValidationError: ... llm Field required\"\n",
    "#   - TEÅHÄ°S: `ChatHuggingFace` aracÄ±nÄ±n, doÄŸrudan model bilgileriyle deÄŸil, Ã¶nce\n",
    "#     oluÅŸturulmuÅŸ bir `HuggingFaceEndpoint` nesnesini `llm=` parametresi olarak beklediÄŸi\n",
    "#     tespit edildi. Kod bu yapÄ±ya uygun deÄŸildi.\n",
    "#\n",
    "#   - Ã‡Ã–ZÃœM PLANI: Bu Ã¼Ã§ sorunu Ã§Ã¶zmek iÃ§in kod, temel baÄŸlantÄ±yÄ± `HuggingFaceEndpoint` ile\n",
    "#     kuracak ve ardÄ±ndan bu baÄŸlantÄ±yÄ± `ChatHuggingFace` sarmalayÄ±cÄ±sÄ±na verecek ÅŸekilde\n",
    "#     yeniden dÃ¼zenlendi. Bu, hem doÄŸru gÃ¶rev formatÄ±nÄ± (`conversational`) kullanmamÄ±zÄ± saÄŸladÄ±\n",
    "#     hem de kÃ¼tÃ¼phanenin beklediÄŸi doÄŸru kurulumu gerÃ§ekleÅŸtirdi.\n",
    "\n",
    "# DENEME 3: Hugging Face ile Uzun SÃ¼reli Ã‡alÄ±ÅŸma (AylÄ±k Kredi HatasÄ±)\n",
    "\n",
    "# HEDEF: DÃ¼zeltilmiÅŸ Hugging Face koduyla tÃ¼m veri setini iÅŸlemek.\n",
    "#\n",
    "# KARÅILAÅILAN SORUN: Ã–deme Gerekli HatasÄ± (402 Payment Required)\n",
    "#   - Hata MesajÄ±: \"You have exceeded your monthly included credits for Inference Providers.\"\n",
    "#   - TEÅHÄ°S: Script'in bir sÃ¼re sorunsuz Ã§alÄ±ÅŸtÄ±ktan sonra durmasÄ±, bu sefer de\n",
    "#     Hugging Face'in \"aylÄ±k Ã¼cretsiz kullanÄ±m kredisinin\" bittiÄŸini gÃ¶sterdi.\n",
    "#     Bu, gÃ¼nlÃ¼k sÄ±fÄ±rlanan bir limit deÄŸildi ve bu yolu da tÄ±kamÄ±ÅŸ oldu.\n",
    "#     Bu durum, Ã¼cretsiz katmanlarÄ±n bÃ¼yÃ¼k Ã¶lÃ§ekli toplu iÅŸlemler iÃ§in uygun\n",
    "#     olmadÄ±ÄŸÄ±nÄ±, daha Ã§ok deneme ve dÃ¼ÅŸÃ¼k trafikli uygulamalar iÃ§in tasarlandÄ±ÄŸÄ±nÄ± kanÄ±tladÄ±.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os  \n",
    "import json  \n",
    "import time  \n",
    "\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "print(\" Sentetik Cevap Ãœretici (Hugging Face - GÃ¼venli Mod) BaÅŸlatÄ±lÄ±yor...\")\n",
    "\n",
    "\n",
    "\n",
    "INPUT_FILE_PATH = r\"C:\\Users\\yedis\\Desktop\\yehu\\Github Repo\\MentorMate-SSS\\output\\enriched_dataset.jsonl\"\n",
    "OUTPUT_FILE_PATH = r\"C:\\Users\\yedis\\Desktop\\yehu\\Github Repo\\MentorMate-SSS\\output\\generated_data_hf.jsonl\"\n",
    "VARIANTS_PER_QUESTION = 2 \n",
    "\n",
    "DELAY_SECONDS = 25 \n",
    "\n",
    "HF_MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\" \n",
    "\n",
    "\n",
    "hf_token = os.environ.get('HUGGINGFACE_HUB_TOKEN')\n",
    "if not hf_token:\n",
    "    \n",
    "    raise ValueError(\"Hugging Face API Token bulunamadÄ±! LÃ¼tfen 'HUGGINGFACE_HUB_TOKEN' ortam deÄŸiÅŸkenini ayarlayÄ±n.\")\n",
    "\n",
    "endpoint_llm = HuggingFaceEndpoint(\n",
    "    repo_id=HF_MODEL_NAME,  \n",
    "    huggingfacehub_api_token=hf_token, \n",
    "    temperature=0.7,  \n",
    "    max_new_tokens=256,  \n",
    "    repetition_penalty=1.2,  \n",
    ")\n",
    "\n",
    "llm = ChatHuggingFace(llm=endpoint_llm)\n",
    "\n",
    "print(f\" Hugging Face Endpoint ve '{HF_MODEL_NAME}' modeli yapÄ±landÄ±rÄ±ldÄ±.\")\n",
    "\n",
    "\n",
    "prompt_template_str = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "Sen bir metin yeniden yazma asistanÄ±sÄ±n. GÃ¶revin, verilen bir cevabÄ±, anlamÄ±nÄ± tamamen koruyarak ama farklÄ± kelimelerle yeniden yazmaktÄ±r. Sadece ve sadece yeniden yazÄ±lmÄ±ÅŸ cevabÄ± dÃ¶ndÃ¼r. BaÅŸka hiÃ§bir aÃ§Ä±klama ekleme.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "Orijinal Soru: {soru}\n",
    "Orijinal Cevap: {orijinal_cevap}\n",
    "Yeniden YazÄ±lmÄ±ÅŸ Cevap:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template_str)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "print(\" LangChain zinciri oluÅŸturuldu.\")\n",
    "\n",
    "\n",
    "try:\n",
    "    start_index = 0\n",
    "    if os.path.exists(OUTPUT_FILE_PATH):\n",
    "        with open(OUTPUT_FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "            lines_in_output = len(f.readlines())\n",
    "        start_index = lines_in_output // VARIANTS_PER_QUESTION\n",
    "        print(f\"\\n Mevcut dosyada {lines_in_output} varyasyon bulundu.\")\n",
    "        if start_index > 0:\n",
    "            print(f\" Ä°ÅŸleme {start_index + 1}. sorudan devam edilecek.\")\n",
    "\n",
    "    with open(INPUT_FILE_PATH, 'r', encoding='utf-8') as infile, \\\n",
    "         open(OUTPUT_FILE_PATH, 'a', encoding='utf-8') as outfile:\n",
    "        \n",
    "        lines = infile.readlines()\n",
    "        lines_to_process = lines[start_index:]\n",
    "        \n",
    "        total_lines = len(lines)\n",
    "        print(f\" Orijinal dosyada toplam {total_lines} soru var. Kalan {len(lines_to_process)} soru iÅŸlenecek.\")\n",
    "\n",
    "        if not lines_to_process:\n",
    "            print(\"TÃ¼m sorular zaten iÅŸlenmiÅŸ. Ä°ÅŸlem tamamlandÄ±.\")\n",
    "        else:\n",
    "            print(f\"Her API Ã§aÄŸrÄ±sÄ± arasÄ±nda {DELAY_SECONDS} saniye beklenecektir...\")\n",
    "\n",
    "        for i, line in enumerate(lines_to_process, start=start_index):\n",
    "            original_data = json.loads(line)\n",
    "            question = original_data.get(\"question\")\n",
    "            answer = original_data.get(\"answer\")\n",
    "            if not question or not answer: continue\n",
    "\n",
    "            print(f\"\\n[{i+1}/{total_lines}] Ä°ÅŸleniyor: '{question[:50]}...'\")\n",
    "            for j in range(VARIANTS_PER_QUESTION):\n",
    "                print(f\"  -> Varyasyon {j+1}/{VARIANTS_PER_QUESTION} Ã¼retiliyor...\")\n",
    "                try:\n",
    "                    new_answer = chain.invoke({\n",
    "                        \"soru\": question,\n",
    "                        \"orijinal_cevap\": answer\n",
    "                    })\n",
    "                    \n",
    "                    new_data_entry = {\"question\": question, \"answer\": new_answer.strip()}\n",
    "                    outfile.write(json.dumps(new_data_entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"    !! API HatasÄ±: {e}. Bu varyasyon atlanÄ±yor.\")\n",
    "                    if \"quota\" in str(e).lower() or \"payment required\" in str(e).lower():\n",
    "                        print(\"    !! KOTA LÄ°MÄ°TÄ°NE ULAÅILDI. Ä°ÅŸlem durduruluyor. LÃ¼tfen daha sonra tekrar deneyin.\")\n",
    "                \n",
    "                time.sleep(DELAY_SECONDS)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80 + f\"\\nÄ°ÅLEM TAMAMLANDI! Ãœretilen veriler '{OUTPUT_FILE_PATH}' dosyasÄ±na kaydedildi.\\n\" + \"=\"*80)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nÄ°ÅŸlem durduruldu. Ãœretilen veriler kaydedildi. Limitin sÄ±fÄ±rlanmasÄ±nÄ± bekledikten sonra aynÄ± hÃ¼creyi tekrar Ã§alÄ±ÅŸtÄ±rarak devam edebilirsiniz.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2820b793-74f1-4482-8629-47aa9d9e5e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "INPUT_FILE_PATH = r\"C:\\Users\\yedis\\Desktop\\yehu\\Github Repo\\MentorMate-SSS\\output\\enriched_dataset.jsonl\"\n",
    "REPORT_FILE_PATH = r\"C:\\Users\\yedis\\Desktop\\yehu\\Github Repo\\MentorMate-SSS\\output\\data_quality_report.txt\"\n",
    "\n",
    "answers_to_questions = defaultdict(list)\n",
    "\n",
    "print(f\"'{INPUT_FILE_PATH}' dosyasÄ± analiz ediliyor...\")\n",
    "\n",
    "try:\n",
    "    with open(INPUT_FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                if 'question' in data and 'answer' in data:\n",
    "                    answers_to_questions[data['answer']].append(data['question'])\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"UyarÄ±: Bir satÄ±r okunamadÄ±, geÃ§iliyor.\")\n",
    "                continue\n",
    "\n",
    "    print(\"Analiz tamamlandÄ±. Rapor oluÅŸturuluyor...\")\n",
    "\n",
    "    with open(REPORT_FILE_PATH, 'w', encoding='utf-8') as f:\n",
    "        sorted_answers = sorted(answers_to_questions.items(), key=lambda item: len(item[1]), reverse=True)\n",
    "        \n",
    "        for answer, questions in sorted_answers:\n",
    "            f.write(\"=\"*80 + \"\\n\")\n",
    "            f.write(f\"CEVAP (Toplam {len(questions)} soru iÃ§in kullanÄ±lmÄ±ÅŸ):\\n\")\n",
    "            f.write(f\"{answer}\\n\")\n",
    "            f.write(\"-\" * 20 + \"\\n\")\n",
    "            f.write(\"Bu CevabÄ±n AtandÄ±ÄŸÄ± Sorular:\\n\")\n",
    "            for i, question in enumerate(questions, 1):\n",
    "                f.write(f\"  {i}. {question}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    print(f\"Rapor baÅŸarÄ±yla '{REPORT_FILE_PATH}' dosyasÄ±na kaydedildi.\")\n",
    "    print(\"LÃ¼tfen bu dosyayÄ± aÃ§Ä±p mantÄ±ksÄ±z Soru-Cevap eÅŸleÅŸmelerini kontrol et.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"HATA: Dosya bulunamadÄ±: {INPUT_FILE_PATH}\")\n",
    "except Exception as e:\n",
    "    print(f\"Beklenmedik bir hata oluÅŸtu: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bbeb7b-6e50-4a75-a881-76e916ea1c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "print(\"Sentetik Cevap Ãœretici (Google Gemini Versiyonu) BaÅŸlatÄ±lÄ±yor...\")\n",
    "\n",
    "INPUT_FILE_PATH = r\"C:\\Users\\yedis\\Desktop\\yehu\\Github Repo\\MentorMate-SSS\\output\\enriched_dataset.jsonl\"\n",
    "OUTPUT_FILE_PATH = r\"C:\\Users\\yedis\\Desktop\\yehu\\Github Repo\\MentorMate-SSS\\output\\generated_data_google.jsonl\"\n",
    "VARIANTS_PER_QUESTION = 2 \n",
    "DELAY_SECONDS = 15 \n",
    "LLM_MODEL_NAME = \"models/gemini-2.0-flash\" \n",
    "\n",
    "api_key = os.environ.get('GOOGLE_API_KEY-cosmo')\n",
    "if not api_key:\n",
    "    raise ValueError(\"Google API AnahtarÄ± bulunamadÄ±! LÃ¼tfen 'GOOGLE_API_KEY' ortam deÄŸiÅŸkenini ayarlayÄ±n.\")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=LLM_MODEL_NAME, temperature=0.7, google_api_key=api_key)\n",
    "print(f\" LangChain ve Gemini modeli ({LLM_MODEL_NAME}) veri Ã¼retimi iÃ§in yapÄ±landÄ±rÄ±ldÄ±.\")\n",
    "\n",
    "prompt_template_str = \"\"\"\n",
    "Sen bir metin yeniden yazma asistanÄ±sÄ±n. GÃ¶revin, verilen bir cevabÄ±, anlamÄ±nÄ± tamamen koruyarak ama farklÄ± kelimelerle yeniden yazmaktÄ±r. Sadece ve sadece yeniden yazÄ±lmÄ±ÅŸ cevabÄ± dÃ¶ndÃ¼r. BaÅŸka hiÃ§bir aÃ§Ä±klama ekleme.\n",
    "\n",
    "Orijinal Soru: {soru}\n",
    "Orijinal Cevap: {orijinal_cevap}\n",
    "Yeniden YazÄ±lmÄ±ÅŸ Cevap:\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template_str)\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt | llm | output_parser\n",
    "print(\"LangChain zinciri oluÅŸturuldu.\")\n",
    "\n",
    "start_index = 0\n",
    "if os.path.exists(OUTPUT_FILE_PATH):\n",
    "    with open(OUTPUT_FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "        lines_in_output = len(f.readlines())\n",
    "        start_index = lines_in_output // VARIANTS_PER_QUESTION\n",
    "        print(f\"\\n Mevcut dosyada {lines_in_output} varyasyon bulundu.\")\n",
    "        if start_index > 0:\n",
    "            print(f\" Ä°ÅŸleme {start_index + 1}. sorudan devam edilecek.\")\n",
    "\n",
    "try:\n",
    "    with open(INPUT_FILE_PATH, 'r', encoding='utf-8') as infile, \\\n",
    "         open(OUTPUT_FILE_PATH, 'a', encoding='utf-8') as outfile:\n",
    "        \n",
    "        lines = infile.readlines()\n",
    "        lines_to_process = lines[start_index:]\n",
    "        total_lines = len(lines)\n",
    "        \n",
    "        print(f\" Orijinal dosyada toplam {total_lines} soru var. Kalan {len(lines_to_process)} soru iÅŸlenecek.\")\n",
    "        if not lines_to_process:\n",
    "            print(\"TÃ¼m sorular zaten iÅŸlenmiÅŸ. Ä°ÅŸlem tamamlandÄ±.\")\n",
    "        else:\n",
    "            print(\"Ä°ÅŸlem baÅŸlÄ±yor...\")\n",
    "\n",
    "        for i, line in enumerate(lines_to_process, start=start_index):\n",
    "            original_data = json.loads(line)\n",
    "            question = original_data.get(\"question\")\n",
    "            answer = original_data.get(\"answer\")\n",
    "            if not question or not answer: continue\n",
    "\n",
    "            print(f\"\\n[{i+1}/{total_lines}] Ä°ÅŸleniyor: '{question[:50]}...'\")\n",
    "            for j in range(VARIANTS_PER_QUESTION):\n",
    "                print(f\"  -> Varyasyon {j+1}/{VARIANTS_PER_QUESTION} Ã¼retiliyor...\")\n",
    "                try:\n",
    "                    new_answer = chain.invoke({\"soru\": question, \"orijinal_cevap\": answer})\n",
    "                    new_data_entry = {\"question\": question, \"answer\": new_answer.strip()}\n",
    "                    outfile.write(json.dumps(new_data_entry, ensure_ascii=False) + \"\\n\")\n",
    "                except Exception as e:\n",
    "                    print(f\"    !! API HatasÄ±: {e}. Bu varyasyon atlanÄ±yor.\")\n",
    "                    if \"quota\" in str(e).lower():\n",
    "                        print(\"    !! GÃœNLÃœK KOTA LÄ°MÄ°TÄ°NE ULAÅILDI. LÃ¼tfen yarÄ±n tekrar deneyin.\")\n",
    "                        raise e \n",
    "                \n",
    "                time.sleep(DELAY_SECONDS)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80 + f\"\\n Ä°ÅLEM TAMAMLANDI! Ãœretilen veriler '{OUTPUT_FILE_PATH}' dosyasÄ±na kaydedildi.\\n\" + \"=\"*80)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Ä°ÅŸlem durduruldu: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daef232b-8f50-4a0a-93d6-ddc62edd451c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
