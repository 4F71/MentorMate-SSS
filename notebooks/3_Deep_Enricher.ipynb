{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c5c8796-1ac1-44cf-9bd0-380b4e5f213b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… spaCy modeli yÃ¼klendi\n",
      "âœ… FAISS kullanÄ±labilir\n",
      "\n",
      "âœ… TÃ¼m kÃ¼tÃ¼phaneler yÃ¼klendi!\n",
      "ğŸ“‹ KonfigÃ¼rasyon:\n",
      "  input1: ..\\data\\sss_dataset_augmented.json\n",
      "  input2: ..\\data\\sss_dataset_heavily_augmented.json\n",
      "  out_dir: ../output\n",
      "  n_paraphrases: 30\n",
      "  provider: local\n",
      "  seed: 42\n",
      "  cluster_threshold: 0.3\n",
      "  batch_size: 32\n",
      "================================================================================\n",
      "ğŸš€ DEEP SSS ENRICHER BAÅLATILDI\n",
      "================================================================================\n",
      "ğŸ“‚ YÃ¼kleniyor: ..\\data\\sss_dataset_augmented.json\n",
      "âœ… 14 satÄ±r yÃ¼klendi\n",
      "ğŸ“‚ YÃ¼kleniyor: ..\\data\\sss_dataset_heavily_augmented.json\n",
      "âœ… 14 satÄ±r yÃ¼klendi\n",
      "\n",
      "ğŸ”— Veri setleri iÅŸleniyor...\n",
      "Toplam satÄ±r (birleÅŸtirme sonrasÄ±): 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Expanding questions: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:00<00:00, 946.26it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toplam soru (expand sonrasÄ±): 812\n",
      "ğŸ—‘ï¸  Exact tekrar silindi: 56 satÄ±r\n",
      "âœ… TemizlenmiÅŸ veri: 756 satÄ±r\n",
      "\n",
      "ğŸ§  Embedding model yÃ¼kleniyor: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¢ 756 metin iÃ§in embedding Ã¼retiliyor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Batches:   0%|                                                                                  | 0/24 [00:00<?, ?it/s]\u001b[A\n",
      "Batches:   4%|â–ˆâ–ˆâ–ˆ                                                                       | 1/24 [00:00<00:07,  3.27it/s]\u001b[A\n",
      "Batches:   8%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                   | 2/24 [00:00<00:05,  4.08it/s]\u001b[A\n",
      "Batches:  12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                | 3/24 [00:00<00:04,  4.74it/s]\u001b[A\n",
      "Batches:  17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                             | 4/24 [00:00<00:03,  5.23it/s]\u001b[A\n",
      "Batches:  21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                          | 5/24 [00:00<00:03,  5.80it/s]\u001b[A\n",
      "Batches:  25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                       | 6/24 [00:01<00:02,  6.24it/s]\u001b[A\n",
      "Batches:  29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                    | 7/24 [00:01<00:02,  6.63it/s]\u001b[A\n",
      "Batches:  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                 | 8/24 [00:01<00:02,  7.09it/s]\u001b[A\n",
      "Batches:  38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                              | 9/24 [00:01<00:02,  7.44it/s]\u001b[A\n",
      "Batches:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                          | 10/24 [00:01<00:01,  7.91it/s]\u001b[A\n",
      "Batches:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                       | 11/24 [00:01<00:01,  8.24it/s]\u001b[A\n",
      "Batches:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                    | 12/24 [00:01<00:01,  8.50it/s]\u001b[A\n",
      "Batches:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                 | 13/24 [00:01<00:01,  8.76it/s]\u001b[A\n",
      "Batches:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                           | 15/24 [00:02<00:00,  9.67it/s]\u001b[A\n",
      "Batches:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                     | 17/24 [00:02<00:00, 10.27it/s]\u001b[A\n",
      "Batches:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š               | 19/24 [00:02<00:00, 10.70it/s]\u001b[A\n",
      "Batches:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰         | 21/24 [00:02<00:00, 11.33it/s]\u001b[A\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [00:02<00:00,  8.61it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Semantik kÃ¼meleme yapÄ±lÄ±yor (threshold=0.3)...\n",
      "âœ… 93 kÃ¼me oluÅŸturuldu\n",
      "\n",
      "ğŸ“ Canonical sorular oluÅŸturuluyor...\n",
      "âœ… 93 canonical soru oluÅŸturuldu\n",
      "âš ï¸  23 kÃ¼me inceleme gerekiyor\n",
      "\n",
      "ğŸ­ Paraphrase Ã¼retimi (30 varyasyon/soru)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Paraphrasing:   0%|                                      | 0/93 [00:00<?, ?it/s]\u001b[A\n",
      "Paraphrasing:   5%|â–ˆâ–Œ                            | 5/93 [00:00<00:01, 44.27it/s]\u001b[A\n",
      "Paraphrasing:  11%|â–ˆâ–ˆâ–ˆ                          | 10/93 [00:00<00:02, 40.68it/s]\u001b[A\n",
      "Paraphrasing:  16%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹                        | 15/93 [00:00<00:01, 42.88it/s]\u001b[A\n",
      "Paraphrasing:  22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 20/93 [00:00<00:01, 43.38it/s]\u001b[A\n",
      "Paraphrasing:  27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                     | 25/93 [00:00<00:01, 40.07it/s]\u001b[A\n",
      "Paraphrasing:  32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 30/93 [00:00<00:01, 37.02it/s]\u001b[A\n",
      "Paraphrasing:  37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                  | 34/93 [00:00<00:01, 37.78it/s]\u001b[A\n",
      "Paraphrasing:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 39/93 [00:00<00:01, 39.65it/s]\u001b[A\n",
      "Paraphrasing:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹               | 44/93 [00:01<00:01, 35.82it/s]\u001b[A\n",
      "Paraphrasing:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰              | 48/93 [00:01<00:01, 32.16it/s]\u001b[A\n",
      "Paraphrasing:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 52/93 [00:01<00:01, 31.62it/s]\u001b[A\n",
      "Paraphrasing:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 57/93 [00:01<00:01, 34.34it/s]\u001b[A\n",
      "Paraphrasing:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 61/93 [00:01<00:00, 33.03it/s]\u001b[A\n",
      "Paraphrasing:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 65/93 [00:01<00:00, 34.36it/s]\u001b[A\n",
      "Paraphrasing:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 69/93 [00:01<00:00, 32.26it/s]\u001b[A\n",
      "Paraphrasing:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š      | 73/93 [00:02<00:00, 33.16it/s]\u001b[A\n",
      "Paraphrasing:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 77/93 [00:02<00:00, 30.44it/s]\u001b[A\n",
      "Paraphrasing:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 81/93 [00:02<00:00, 32.24it/s]\u001b[A\n",
      "Paraphrasing:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 86/93 [00:02<00:00, 34.74it/s]\u001b[A\n",
      "Paraphrasing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 93/93 [00:02<00:00, 34.81it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 255 yeni paraphrase oluÅŸturuldu\n",
      "\n",
      "ğŸ“Š Toplam satÄ±r (paraphrase sonrasÄ±): 1011\n",
      "\n",
      "ğŸ·ï¸  Metadata zenginleÅŸtiriliyor...\n",
      "âœ… Metadata zenginleÅŸtirme tamamlandÄ±\n",
      "\n",
      "ğŸ’¾ TÃ¼m Ã§Ä±ktÄ±lar kaydedildi!\n",
      "================================================================================\n",
      "âœ… Ä°ÅLEM TAMAMLANDI\n",
      "================================================================================\n",
      "ğŸ“Š Toplam kayÄ±t: 1,011\n",
      "ğŸ“ Orijinal kayÄ±t: 0\n",
      "ğŸ­ Paraphrase kayÄ±t: 255\n",
      "ğŸ” KÃ¼me sayÄ±sÄ±: 93\n",
      "â­ Canonical soru: 93\n",
      "âš ï¸  Ä°nceleme gereken: 23\n",
      "â±ï¸  Ä°ÅŸlem sÃ¼resi: 8.03 saniye\n",
      "ğŸ“ Ã‡Ä±ktÄ± dizini: ../output\n",
      "================================================================================\n",
      "\n",
      "ğŸ“‹ Ã–rnek Canonical Sorular:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster_id</th>\n",
       "      <th>canonical_question</th>\n",
       "      <th>canonical_answer</th>\n",
       "      <th>category</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>**Bootcamp SertifikasÄ± Alacak mÄ±yÄ±m?**</td>\n",
       "      <td>Evet, bootcamp sonunda projenizi baÅŸarÄ±yla tam...</td>\n",
       "      <td>Genel</td>\n",
       "      <td>bootcamp, iÅŸ imkanÄ±, staj imkanÄ±, bootcamp iÅŸ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>22.0</td>\n",
       "      <td>CanlÄ± yayÄ±nlar arÅŸivleniyor mu?</td>\n",
       "      <td>Evet, tÃ¼m canlÄ± yayÄ±nlar kaydedilmekte ve **Yo...</td>\n",
       "      <td>Genel</td>\n",
       "      <td>canlÄ± yayÄ±n yoklama, katÄ±lÄ±m belgesi, canlÄ± ya...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16.0</td>\n",
       "      <td>Bu projeyi takÄ±m halinde mi gerÃ§ekleÅŸtirmeliyiz?</td>\n",
       "      <td>Projeleri tek baÅŸÄ±nÄ±za ya da en fazla 2 kiÅŸili...</td>\n",
       "      <td>Genel</td>\n",
       "      <td>grup projesi, tek kiÅŸilik proje, proje grup zo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Grup kurmak zorunda mÄ±yÄ±m?</td>\n",
       "      <td>Projeleri tek baÅŸÄ±nÄ±za ya da en fazla 2 kiÅŸili...</td>\n",
       "      <td>Genel</td>\n",
       "      <td>grup projesi, tek kiÅŸilik proje, proje grup zo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>69.0</td>\n",
       "      <td>Projeyi ekipÃ§e yÃ¼rÃ¼tmemiz mi bekleniyor?</td>\n",
       "      <td>Projeleri tek baÅŸÄ±nÄ±za ya da en fazla 2 kiÅŸili...</td>\n",
       "      <td>Genel</td>\n",
       "      <td>grup projesi, tek kiÅŸilik proje, proje grup zo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Mentor buluÅŸmalarÄ±na katÄ±lmak faydalÄ± mÄ±?</td>\n",
       "      <td>Mentor toplantÄ±larÄ± ve saatleri, Zulip'teki du...</td>\n",
       "      <td>Genel</td>\n",
       "      <td>mentor toplantÄ±larÄ±, mentor toplantÄ± saatleri,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>9.0</td>\n",
       "      <td>Kodumu GitHub'a hÄ±zlÄ±ca nasÄ±l yÃ¼kleyebilirim?</td>\n",
       "      <td>Web arayÃ¼zÃ¼ ile yÃ¼kleme: https://www.youtube.c...</td>\n",
       "      <td>Genel</td>\n",
       "      <td>GitHub'a kod yÃ¼kleme, Git kullanmadan GitHub, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>37.0</td>\n",
       "      <td>YouTube kanalÄ±nÄ±zÄ±n adÄ± nedir?</td>\n",
       "      <td>Evet, tÃ¼m canlÄ± yayÄ±nlar kaydedilmekte ve **Yo...</td>\n",
       "      <td>Genel</td>\n",
       "      <td>Youtube CanlÄ± YayÄ±n, CanlÄ± YayÄ±n Yoklama, KatÄ±...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>71.0</td>\n",
       "      <td>Sertifika'da Ä°sim-Harf Problemi</td>\n",
       "      <td>Merhaba, BazÄ± kullanÄ±cÄ±larÄ±mÄ±z sertifikalarÄ±nd...</td>\n",
       "      <td>Genel</td>\n",
       "      <td>sertifika isim hatasÄ±, sertifika karakter soru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>91.0</td>\n",
       "      <td>Belgelerdeki ad yazÄ±mÄ±nda harf yanlÄ±ÅŸlÄ±ÄŸÄ±</td>\n",
       "      <td>Merhaba, BazÄ± kullanÄ±cÄ±larÄ±mÄ±z sertifikalarÄ±nd...</td>\n",
       "      <td>Genel</td>\n",
       "      <td>sertifika isim hatasÄ±, sertifika karakter soru...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cluster_id                                canonical_question  \\\n",
       "0          0.0            **Bootcamp SertifikasÄ± Alacak mÄ±yÄ±m?**   \n",
       "8         22.0                   CanlÄ± yayÄ±nlar arÅŸivleniyor mu?   \n",
       "17        16.0  Bu projeyi takÄ±m halinde mi gerÃ§ekleÅŸtirmeliyiz?   \n",
       "18         2.0                        Grup kurmak zorunda mÄ±yÄ±m?   \n",
       "19        69.0          Projeyi ekipÃ§e yÃ¼rÃ¼tmemiz mi bekleniyor?   \n",
       "20         1.0         Mentor buluÅŸmalarÄ±na katÄ±lmak faydalÄ± mÄ±?   \n",
       "28         9.0     Kodumu GitHub'a hÄ±zlÄ±ca nasÄ±l yÃ¼kleyebilirim?   \n",
       "32        37.0                    YouTube kanalÄ±nÄ±zÄ±n adÄ± nedir?   \n",
       "36        71.0                   Sertifika'da Ä°sim-Harf Problemi   \n",
       "38        91.0         Belgelerdeki ad yazÄ±mÄ±nda harf yanlÄ±ÅŸlÄ±ÄŸÄ±   \n",
       "\n",
       "                                     canonical_answer category  \\\n",
       "0   Evet, bootcamp sonunda projenizi baÅŸarÄ±yla tam...    Genel   \n",
       "8   Evet, tÃ¼m canlÄ± yayÄ±nlar kaydedilmekte ve **Yo...    Genel   \n",
       "17  Projeleri tek baÅŸÄ±nÄ±za ya da en fazla 2 kiÅŸili...    Genel   \n",
       "18  Projeleri tek baÅŸÄ±nÄ±za ya da en fazla 2 kiÅŸili...    Genel   \n",
       "19  Projeleri tek baÅŸÄ±nÄ±za ya da en fazla 2 kiÅŸili...    Genel   \n",
       "20  Mentor toplantÄ±larÄ± ve saatleri, Zulip'teki du...    Genel   \n",
       "28  Web arayÃ¼zÃ¼ ile yÃ¼kleme: https://www.youtube.c...    Genel   \n",
       "32  Evet, tÃ¼m canlÄ± yayÄ±nlar kaydedilmekte ve **Yo...    Genel   \n",
       "36  Merhaba, BazÄ± kullanÄ±cÄ±larÄ±mÄ±z sertifikalarÄ±nd...    Genel   \n",
       "38  Merhaba, BazÄ± kullanÄ±cÄ±larÄ±mÄ±z sertifikalarÄ±nd...    Genel   \n",
       "\n",
       "                                             keywords  \n",
       "0   bootcamp, iÅŸ imkanÄ±, staj imkanÄ±, bootcamp iÅŸ ...  \n",
       "8   canlÄ± yayÄ±n yoklama, katÄ±lÄ±m belgesi, canlÄ± ya...  \n",
       "17  grup projesi, tek kiÅŸilik proje, proje grup zo...  \n",
       "18  grup projesi, tek kiÅŸilik proje, proje grup zo...  \n",
       "19  grup projesi, tek kiÅŸilik proje, proje grup zo...  \n",
       "20  mentor toplantÄ±larÄ±, mentor toplantÄ± saatleri,...  \n",
       "28  GitHub'a kod yÃ¼kleme, Git kullanmadan GitHub, ...  \n",
       "32  Youtube CanlÄ± YayÄ±n, CanlÄ± YayÄ±n Yoklama, KatÄ±...  \n",
       "36  sertifika isim hatasÄ±, sertifika karakter soru...  \n",
       "38  sertifika isim hatasÄ±, sertifika karakter soru...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“‹ Ã–rnek Paraphrase'ler:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>original_question</th>\n",
       "      <th>category</th>\n",
       "      <th>keywords</th>\n",
       "      <th>difficulty_level</th>\n",
       "      <th>source</th>\n",
       "      <th>created_at</th>\n",
       "      <th>cluster_id</th>\n",
       "      <th>canonical_question</th>\n",
       "      <th>canonical_answer</th>\n",
       "      <th>cluster_size</th>\n",
       "      <th>is_canonical</th>\n",
       "      <th>canonical_id</th>\n",
       "      <th>augmentation_method</th>\n",
       "      <th>confidence</th>\n",
       "      <th>id</th>\n",
       "      <th>intent</th>\n",
       "      <th>entities</th>\n",
       "      <th>difficulty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>**bootcamp sertifikasÄ± alacak mÄ±yÄ±m?**?</td>\n",
       "      <td>Evet, bootcamp sonunda projenizi baÅŸarÄ±yla tam...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>courses</td>\n",
       "      <td>sertifikasÄ±, alacak, bootcamp, mÄ±yÄ±m</td>\n",
       "      <td>NaN</td>\n",
       "      <td>generated</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>**Bootcamp SertifikasÄ± Alacak mÄ±yÄ±m?**</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>paraphrase_local</td>\n",
       "      <td>0.787344</td>\n",
       "      <td>140d4588eb60f4d5</td>\n",
       "      <td>information</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757</th>\n",
       "      <td>**bootcamp sertifikasÄ± alacak mÄ±yÄ±m?** hakkÄ±nd...</td>\n",
       "      <td>Evet, bootcamp sonunda projenizi baÅŸarÄ±yla tam...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>courses</td>\n",
       "      <td>verir, mÄ±yÄ±m, misiniz, alacak, sertifikasÄ±</td>\n",
       "      <td>NaN</td>\n",
       "      <td>generated</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>**Bootcamp SertifikasÄ± Alacak mÄ±yÄ±m?**</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>paraphrase_local</td>\n",
       "      <td>0.750345</td>\n",
       "      <td>de5e6100b415ffce</td>\n",
       "      <td>information</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>**bootcamp sertifikasÄ± alacak mÄ±yÄ±m?** konusun...</td>\n",
       "      <td>Evet, bootcamp sonunda projenizi baÅŸarÄ±yla tam...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>courses</td>\n",
       "      <td>konusunda, olabilir, mÄ±yÄ±m, yardÄ±mcÄ±, misiniz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>generated</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>**Bootcamp SertifikasÄ± Alacak mÄ±yÄ±m?**</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>paraphrase_local</td>\n",
       "      <td>0.740952</td>\n",
       "      <td>43dc39c20ac1ecc9</td>\n",
       "      <td>information</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>**bootcamp sertifikasÄ± alacak mÄ±yÄ±m?**</td>\n",
       "      <td>Evet, bootcamp sonunda projenizi baÅŸarÄ±yla tam...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>courses</td>\n",
       "      <td>sertifikasÄ±, alacak, bootcamp, mÄ±yÄ±m</td>\n",
       "      <td>NaN</td>\n",
       "      <td>generated</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>**Bootcamp SertifikasÄ± Alacak mÄ±yÄ±m?**</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>paraphrase_local</td>\n",
       "      <td>0.848164</td>\n",
       "      <td>46da9f877cc008bc</td>\n",
       "      <td>information</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>CanlÄ± yayÄ±nlar arÅŸivleniyor mu??</td>\n",
       "      <td>Evet, tÃ¼m canlÄ± yayÄ±nlar kaydedilmekte ve **Yo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>general</td>\n",
       "      <td>arÅŸivleniyor, yayÄ±nlar, canlÄ±</td>\n",
       "      <td>NaN</td>\n",
       "      <td>generated</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CanlÄ± yayÄ±nlar arÅŸivleniyor mu?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>22.0</td>\n",
       "      <td>paraphrase_local</td>\n",
       "      <td>0.866781</td>\n",
       "      <td>8e5dded6322d5a7f</td>\n",
       "      <td>information</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>CanlÄ± yayÄ±nlar arÅŸivleniyor mu? hakkÄ±nda bilgi...</td>\n",
       "      <td>Evet, tÃ¼m canlÄ± yayÄ±nlar kaydedilmekte ve **Yo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>general</td>\n",
       "      <td>verir, arÅŸivleniyor, yayÄ±nlar, canlÄ±, misiniz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>generated</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CanlÄ± yayÄ±nlar arÅŸivleniyor mu?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>22.0</td>\n",
       "      <td>paraphrase_local</td>\n",
       "      <td>0.798673</td>\n",
       "      <td>d6b30092cedc3ab5</td>\n",
       "      <td>information</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>CanlÄ± yayÄ±nlar arÅŸivleniyor mu? konusunda yard...</td>\n",
       "      <td>Evet, tÃ¼m canlÄ± yayÄ±nlar kaydedilmekte ve **Yo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>support</td>\n",
       "      <td>konusunda, olabilir, arÅŸivleniyor, yayÄ±nlar, c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>generated</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CanlÄ± yayÄ±nlar arÅŸivleniyor mu?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>22.0</td>\n",
       "      <td>paraphrase_local</td>\n",
       "      <td>0.790438</td>\n",
       "      <td>de9a60a0749731ce</td>\n",
       "      <td>information</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>CanlÄ± yayÄ±nlar arÅŸivleniyor mu?</td>\n",
       "      <td>Evet, tÃ¼m canlÄ± yayÄ±nlar kaydedilmekte ve **Yo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>general</td>\n",
       "      <td>arÅŸivleniyor, yayÄ±nlar, canlÄ±</td>\n",
       "      <td>NaN</td>\n",
       "      <td>generated</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CanlÄ± yayÄ±nlar arÅŸivleniyor mu?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>22.0</td>\n",
       "      <td>paraphrase_local</td>\n",
       "      <td>0.859113</td>\n",
       "      <td>059091a1c6046881</td>\n",
       "      <td>information</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>Bu projeyi takÄ±m halinde mi gerÃ§ekleÅŸtirmeliyi...</td>\n",
       "      <td>Projeleri tek baÅŸÄ±nÄ±za ya da en fazla 2 kiÅŸili...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>general</td>\n",
       "      <td>verir, projeyi, takÄ±m, halinde, misiniz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>generated</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bu projeyi takÄ±m halinde mi gerÃ§ekleÅŸtirmeliyiz?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>16.0</td>\n",
       "      <td>paraphrase_local</td>\n",
       "      <td>0.921963</td>\n",
       "      <td>203802ce140d2a32</td>\n",
       "      <td>information</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>Bu projeyi takÄ±m halinde mi gerÃ§ekleÅŸtirmeliyi...</td>\n",
       "      <td>Projeleri tek baÅŸÄ±nÄ±za ya da en fazla 2 kiÅŸili...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>support</td>\n",
       "      <td>konusunda, projeyi, olabilir, takÄ±m, halinde</td>\n",
       "      <td>NaN</td>\n",
       "      <td>generated</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bu projeyi takÄ±m halinde mi gerÃ§ekleÅŸtirmeliyiz?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>16.0</td>\n",
       "      <td>paraphrase_local</td>\n",
       "      <td>0.901847</td>\n",
       "      <td>4f554fa9ea0c6cb6</td>\n",
       "      <td>information</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question  \\\n",
       "756            **bootcamp sertifikasÄ± alacak mÄ±yÄ±m?**?   \n",
       "757  **bootcamp sertifikasÄ± alacak mÄ±yÄ±m?** hakkÄ±nd...   \n",
       "758  **bootcamp sertifikasÄ± alacak mÄ±yÄ±m?** konusun...   \n",
       "759             **bootcamp sertifikasÄ± alacak mÄ±yÄ±m?**   \n",
       "760                   CanlÄ± yayÄ±nlar arÅŸivleniyor mu??   \n",
       "761  CanlÄ± yayÄ±nlar arÅŸivleniyor mu? hakkÄ±nda bilgi...   \n",
       "762  CanlÄ± yayÄ±nlar arÅŸivleniyor mu? konusunda yard...   \n",
       "763                    CanlÄ± yayÄ±nlar arÅŸivleniyor mu?   \n",
       "764  Bu projeyi takÄ±m halinde mi gerÃ§ekleÅŸtirmeliyi...   \n",
       "765  Bu projeyi takÄ±m halinde mi gerÃ§ekleÅŸtirmeliyi...   \n",
       "\n",
       "                                                answer original_question  \\\n",
       "756  Evet, bootcamp sonunda projenizi baÅŸarÄ±yla tam...               NaN   \n",
       "757  Evet, bootcamp sonunda projenizi baÅŸarÄ±yla tam...               NaN   \n",
       "758  Evet, bootcamp sonunda projenizi baÅŸarÄ±yla tam...               NaN   \n",
       "759  Evet, bootcamp sonunda projenizi baÅŸarÄ±yla tam...               NaN   \n",
       "760  Evet, tÃ¼m canlÄ± yayÄ±nlar kaydedilmekte ve **Yo...               NaN   \n",
       "761  Evet, tÃ¼m canlÄ± yayÄ±nlar kaydedilmekte ve **Yo...               NaN   \n",
       "762  Evet, tÃ¼m canlÄ± yayÄ±nlar kaydedilmekte ve **Yo...               NaN   \n",
       "763  Evet, tÃ¼m canlÄ± yayÄ±nlar kaydedilmekte ve **Yo...               NaN   \n",
       "764  Projeleri tek baÅŸÄ±nÄ±za ya da en fazla 2 kiÅŸili...               NaN   \n",
       "765  Projeleri tek baÅŸÄ±nÄ±za ya da en fazla 2 kiÅŸili...               NaN   \n",
       "\n",
       "    category                                           keywords  \\\n",
       "756  courses               sertifikasÄ±, alacak, bootcamp, mÄ±yÄ±m   \n",
       "757  courses         verir, mÄ±yÄ±m, misiniz, alacak, sertifikasÄ±   \n",
       "758  courses      konusunda, olabilir, mÄ±yÄ±m, yardÄ±mcÄ±, misiniz   \n",
       "759  courses               sertifikasÄ±, alacak, bootcamp, mÄ±yÄ±m   \n",
       "760  general                      arÅŸivleniyor, yayÄ±nlar, canlÄ±   \n",
       "761  general      verir, arÅŸivleniyor, yayÄ±nlar, canlÄ±, misiniz   \n",
       "762  support  konusunda, olabilir, arÅŸivleniyor, yayÄ±nlar, c...   \n",
       "763  general                      arÅŸivleniyor, yayÄ±nlar, canlÄ±   \n",
       "764  general            verir, projeyi, takÄ±m, halinde, misiniz   \n",
       "765  support       konusunda, projeyi, olabilir, takÄ±m, halinde   \n",
       "\n",
       "    difficulty_level     source created_at  cluster_id  \\\n",
       "756              NaN  generated        NaN         NaN   \n",
       "757              NaN  generated        NaN         NaN   \n",
       "758              NaN  generated        NaN         NaN   \n",
       "759              NaN  generated        NaN         NaN   \n",
       "760              NaN  generated        NaN         NaN   \n",
       "761              NaN  generated        NaN         NaN   \n",
       "762              NaN  generated        NaN         NaN   \n",
       "763              NaN  generated        NaN         NaN   \n",
       "764              NaN  generated        NaN         NaN   \n",
       "765              NaN  generated        NaN         NaN   \n",
       "\n",
       "                                   canonical_question canonical_answer  \\\n",
       "756            **Bootcamp SertifikasÄ± Alacak mÄ±yÄ±m?**              NaN   \n",
       "757            **Bootcamp SertifikasÄ± Alacak mÄ±yÄ±m?**              NaN   \n",
       "758            **Bootcamp SertifikasÄ± Alacak mÄ±yÄ±m?**              NaN   \n",
       "759            **Bootcamp SertifikasÄ± Alacak mÄ±yÄ±m?**              NaN   \n",
       "760                   CanlÄ± yayÄ±nlar arÅŸivleniyor mu?              NaN   \n",
       "761                   CanlÄ± yayÄ±nlar arÅŸivleniyor mu?              NaN   \n",
       "762                   CanlÄ± yayÄ±nlar arÅŸivleniyor mu?              NaN   \n",
       "763                   CanlÄ± yayÄ±nlar arÅŸivleniyor mu?              NaN   \n",
       "764  Bu projeyi takÄ±m halinde mi gerÃ§ekleÅŸtirmeliyiz?              NaN   \n",
       "765  Bu projeyi takÄ±m halinde mi gerÃ§ekleÅŸtirmeliyiz?              NaN   \n",
       "\n",
       "     cluster_size  is_canonical  canonical_id augmentation_method  confidence  \\\n",
       "756           NaN         False           0.0    paraphrase_local    0.787344   \n",
       "757           NaN         False           0.0    paraphrase_local    0.750345   \n",
       "758           NaN         False           0.0    paraphrase_local    0.740952   \n",
       "759           NaN         False           0.0    paraphrase_local    0.848164   \n",
       "760           NaN         False          22.0    paraphrase_local    0.866781   \n",
       "761           NaN         False          22.0    paraphrase_local    0.798673   \n",
       "762           NaN         False          22.0    paraphrase_local    0.790438   \n",
       "763           NaN         False          22.0    paraphrase_local    0.859113   \n",
       "764           NaN         False          16.0    paraphrase_local    0.921963   \n",
       "765           NaN         False          16.0    paraphrase_local    0.901847   \n",
       "\n",
       "                   id       intent entities difficulty  \n",
       "756  140d4588eb60f4d5  information                 NaN  \n",
       "757  de5e6100b415ffce  information                 NaN  \n",
       "758  43dc39c20ac1ecc9  information                 NaN  \n",
       "759  46da9f877cc008bc  information                 NaN  \n",
       "760  8e5dded6322d5a7f  information                 NaN  \n",
       "761  d6b30092cedc3ab5  information                 NaN  \n",
       "762  de9a60a0749731ce  information                 NaN  \n",
       "763  059091a1c6046881  information                 NaN  \n",
       "764  203802ce140d2a32  information                 NaN  \n",
       "765  4f554fa9ea0c6cb6  information                 NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Kategori DaÄŸÄ±lÄ±mÄ±:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "category\n",
       "Genel            756\n",
       "general          126\n",
       "support           69\n",
       "certification     38\n",
       "account           14\n",
       "courses            8\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Intent DaÄŸÄ±lÄ±mÄ±:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "intent\n",
       "information    717\n",
       "how_to         157\n",
       "definition     111\n",
       "reasoning       18\n",
       "timing           8\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Deep SSS Enricher - Jupyter Notebook Versiyonu\n",
    "# \n",
    "# Bu notebook, iki SSS veri setini birleÅŸtirir, temizler, zenginleÅŸtirir ve RAG-ready hale getirir.\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Kurulum ve Import'lar\n",
    "\n",
    "# %%\n",
    "# Gerekli kÃ¼tÃ¼phanelerin kurulumu (ilk Ã§alÄ±ÅŸtÄ±rmada bir kez Ã§alÄ±ÅŸtÄ±rÄ±n)\n",
    "# !pip install pandas numpy tqdm sentence-transformers scikit-learn transformers requests faiss-cpu spacy anthropic openai\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import time\n",
    "import re\n",
    "import hashlib\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm  # Standart tqdm kullan\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "try:\n",
    "    import spacy\n",
    "    try:\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        print(\"âœ… spaCy modeli yÃ¼klendi\")\n",
    "    except:\n",
    "        print(\"âš ï¸  spaCy modeli bulunamadÄ±, basit mod aktif\")\n",
    "        nlp = None\n",
    "except ImportError:\n",
    "    nlp = None\n",
    "    print(\"âš ï¸  spaCy kurulu deÄŸil\")\n",
    "\n",
    "import requests\n",
    "\n",
    "try:\n",
    "    import faiss\n",
    "    FAISS_AVAILABLE = True\n",
    "    print(\"âœ… FAISS kullanÄ±labilir\")\n",
    "except:\n",
    "    FAISS_AVAILABLE = False\n",
    "    print(\"âš ï¸  FAISS yÃ¼klÃ¼ deÄŸil\")\n",
    "\n",
    "print(\"\\nâœ… TÃ¼m kÃ¼tÃ¼phaneler yÃ¼klendi!\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. KonfigÃ¼rasyon\n",
    "\n",
    "# %%\n",
    "# AYARLAR - Buradan deÄŸiÅŸtirebilirsiniz\n",
    "CONFIG = {\n",
    "    'input1': r'..\\data\\sss_dataset_augmented.json',\n",
    "    'input2': r'..\\data\\sss_dataset_heavily_augmented.json',\n",
    "    'out_dir': '../output',\n",
    "    'n_paraphrases': 30,\n",
    "    'provider': 'local',  # 'local', 'anthropic', veya 'openai'\n",
    "    'api_key': None,  # API key varsa buraya yazÄ±n\n",
    "    'seed': 42,\n",
    "    'cluster_threshold': 0.3,\n",
    "    'batch_size': 32\n",
    "}\n",
    "\n",
    "print(\"ğŸ“‹ KonfigÃ¼rasyon:\")\n",
    "for key, value in CONFIG.items():\n",
    "    if key != 'api_key':\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. YardÄ±mcÄ± Fonksiyonlar\n",
    "\n",
    "# %%\n",
    "def setup_logger(log_dir: str) -> logging.Logger:\n",
    "    \"\"\"Log yapÄ±landÄ±rmasÄ±\"\"\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    log_file = os.path.join(log_dir, f\"enricher_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\")\n",
    "    \n",
    "    logger = logging.getLogger(\"DeepEnricher\")\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.handlers = []  # Ã–nceki handler'larÄ± temizle\n",
    "    \n",
    "    fh = logging.FileHandler(log_file, encoding='utf-8')\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    fh.setFormatter(formatter)\n",
    "    logger.addHandler(fh)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"Metin normalizasyonu\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text).strip()\n",
    "    text = text.encode('utf-8', 'ignore').decode('utf-8')\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "# %%\n",
    "def load_dataset(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"DosyayÄ± otomatik algÄ±layarak yÃ¼kle\"\"\"\n",
    "    print(f\"ğŸ“‚ YÃ¼kleniyor: {file_path}\")\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Dosya bulunamadÄ±: {file_path}\")\n",
    "    \n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    \n",
    "    if ext == '.csv':\n",
    "        df = pd.read_csv(file_path, encoding='utf-8')\n",
    "    elif ext == '.jsonl':\n",
    "        df = pd.read_json(file_path, lines=True, encoding='utf-8')\n",
    "    elif ext == '.json':\n",
    "        try:\n",
    "            df = pd.read_json(file_path, encoding='utf-8')\n",
    "        except:\n",
    "            df = pd.read_json(file_path, lines=True, encoding='utf-8')\n",
    "    else:\n",
    "        raise ValueError(f\"Desteklenmeyen format: {ext}\")\n",
    "    \n",
    "    print(f\"âœ… {len(df)} satÄ±r yÃ¼klendi\")\n",
    "    return df\n",
    "\n",
    "def merge_and_deduplicate(df1: pd.DataFrame, df2: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Ä°ki veri setini birleÅŸtir ve tekrarlarÄ± sil\"\"\"\n",
    "    print(\"\\nğŸ”— Veri setleri iÅŸleniyor...\")\n",
    "    \n",
    "    # BirleÅŸtir\n",
    "    combined = pd.concat([df1, df2], ignore_index=True)\n",
    "    print(f\"Toplam satÄ±r (birleÅŸtirme sonrasÄ±): {len(combined)}\")\n",
    "    \n",
    "    # SÃ¼tun adlarÄ±nÄ± normalize et\n",
    "    combined.columns = combined.columns.str.lower().str.strip()\n",
    "    \n",
    "    # all_questions'Ä± expand et (her soruyu ayrÄ± satÄ±r yap)\n",
    "    expanded_rows = []\n",
    "    \n",
    "    for idx, row in tqdm(combined.iterrows(), total=len(combined), desc=\"Expanding questions\"):\n",
    "        all_questions = row['all_questions']\n",
    "        \n",
    "        # Liste formatÄ±nÄ± kontrol et\n",
    "        if isinstance(all_questions, str):\n",
    "            try:\n",
    "                all_questions = eval(all_questions)\n",
    "            except:\n",
    "                all_questions = [all_questions]\n",
    "        \n",
    "        # Dict listesi mi yoksa string listesi mi?\n",
    "        if isinstance(all_questions, list) and len(all_questions) > 0:\n",
    "            if isinstance(all_questions[0], dict):\n",
    "                # Dict formatÄ±: [{'question': '...', 'method': '...'}]\n",
    "                questions = [q.get('question', q.get('text', '')) for q in all_questions]\n",
    "            else:\n",
    "                # String listesi\n",
    "                questions = all_questions\n",
    "        else:\n",
    "            questions = [row.get('original_question', '')]\n",
    "        \n",
    "        # Her soru iÃ§in ayrÄ± satÄ±r oluÅŸtur\n",
    "        for question in questions:\n",
    "            expanded_rows.append({\n",
    "                'question': normalize_text(str(question)),\n",
    "                'answer': normalize_text(str(row['answer'])),\n",
    "                'original_question': normalize_text(str(row['original_question'])),\n",
    "                'category': row.get('category', 'general'),\n",
    "                'keywords': row.get('keywords', ''),\n",
    "                'difficulty_level': row.get('difficulty_level', 'medium'),\n",
    "                'source': 'original'\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(expanded_rows)\n",
    "    print(f\"Toplam soru (expand sonrasÄ±): {len(df)}\")\n",
    "    \n",
    "    # BoÅŸ sorularÄ± sil\n",
    "    df = df[df['question'].str.len() > 0]\n",
    "    df = df[df['answer'].str.len() > 0]\n",
    "    \n",
    "    # Exact deduplication\n",
    "    initial_count = len(df)\n",
    "    df = df.drop_duplicates(subset=['question', 'answer'], keep='first')\n",
    "    print(f\"ğŸ—‘ï¸  Exact tekrar silindi: {initial_count - len(df)} satÄ±r\")\n",
    "    \n",
    "    # Created at ekle\n",
    "    df['created_at'] = datetime.now().isoformat()\n",
    "    \n",
    "    df = df.reset_index(drop=True)\n",
    "    print(f\"âœ… TemizlenmiÅŸ veri: {len(df)} satÄ±r\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Embedding ve KÃ¼meleme\n",
    "\n",
    "# %%\n",
    "def create_embeddings(texts: List[str], model_name: str = 'sentence-transformers/all-MiniLM-L6-v2') -> np.ndarray:\n",
    "    \"\"\"Embedding oluÅŸtur\"\"\"\n",
    "    print(f\"\\nğŸ§  Embedding model yÃ¼kleniyor: {model_name}\")\n",
    "    model = SentenceTransformer(model_name)\n",
    "    \n",
    "    print(f\"ğŸ”¢ {len(texts)} metin iÃ§in embedding Ã¼retiliyor...\")\n",
    "    embeddings = model.encode(texts, show_progress_bar=True, batch_size=32)\n",
    "    \n",
    "    return embeddings, model\n",
    "\n",
    "def cluster_questions(df: pd.DataFrame, embeddings: np.ndarray, threshold: float) -> pd.DataFrame:\n",
    "    \"\"\"Semantik kÃ¼meleme\"\"\"\n",
    "    print(f\"\\nğŸ” Semantik kÃ¼meleme yapÄ±lÄ±yor (threshold={threshold})...\")\n",
    "    \n",
    "    clustering = AgglomerativeClustering(\n",
    "        n_clusters=None,\n",
    "        distance_threshold=threshold,\n",
    "        linkage='average',\n",
    "        metric='cosine'\n",
    "    )\n",
    "    \n",
    "    clusters = clustering.fit_predict(embeddings)\n",
    "    df['cluster_id'] = clusters\n",
    "    \n",
    "    n_clusters = len(set(clusters))\n",
    "    print(f\"âœ… {n_clusters} kÃ¼me oluÅŸturuldu\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def select_canonical(group: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"KÃ¼me iÃ§in canonical soru/cevap seÃ§\"\"\"\n",
    "    group = group.copy()\n",
    "    group['q_len'] = group['question'].str.len()\n",
    "    canonical_row = group.sort_values('q_len').iloc[0]\n",
    "    \n",
    "    unique_answers = group['answer'].unique()\n",
    "    \n",
    "    if len(unique_answers) == 1:\n",
    "        canonical_answer = unique_answers[0]\n",
    "        needs_review = False\n",
    "    else:\n",
    "        canonical_answer = canonical_row['answer']\n",
    "        needs_review = True\n",
    "    \n",
    "    return pd.Series({\n",
    "        'canonical_question': canonical_row['question'],\n",
    "        'canonical_answer': canonical_answer,\n",
    "        'cluster_size': len(group),\n",
    "        'needs_review': needs_review,\n",
    "        'initial_sources': '; '.join(group['source'].unique())\n",
    "    })\n",
    "\n",
    "def canonicalize_clusters(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Canonical sorular oluÅŸtur\"\"\"\n",
    "    print(\"\\nğŸ“ Canonical sorular oluÅŸturuluyor...\")\n",
    "    \n",
    "    canonical_df = df.groupby('cluster_id').apply(select_canonical).reset_index()\n",
    "    review_needed = canonical_df[canonical_df['needs_review'] == True].copy()\n",
    "    \n",
    "    df = df.merge(\n",
    "        canonical_df[['cluster_id', 'canonical_question', 'canonical_answer', 'cluster_size']],\n",
    "        on='cluster_id',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… {len(canonical_df)} canonical soru oluÅŸturuldu\")\n",
    "    print(f\"âš ï¸  {len(review_needed)} kÃ¼me inceleme gerekiyor\")\n",
    "    \n",
    "    return df, review_needed\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. Paraphrase Ãœretimi\n",
    "\n",
    "# %%\n",
    "class ParaphraseGenerator:\n",
    "    \"\"\"LLM tabanlÄ± paraphrase Ã¼retici\"\"\"\n",
    "    \n",
    "    def __init__(self, provider: str, api_key: str = None, seed: int = 42):\n",
    "        self.provider = provider\n",
    "        self.api_key = api_key\n",
    "        self.seed = seed\n",
    "        \n",
    "    def generate(self, question: str, n: int) -> List[str]:\n",
    "        \"\"\"N adet paraphrase Ã¼ret\"\"\"\n",
    "        if self.provider == 'local':\n",
    "            return self._local_paraphrase(question, n)\n",
    "        elif self.provider == 'anthropic':\n",
    "            return self._anthropic_paraphrase(question, n)\n",
    "        elif self.provider == 'openai':\n",
    "            return self._openai_paraphrase(question, n)\n",
    "        else:\n",
    "            print(f\"âŒ Bilinmeyen provider: {self.provider}\")\n",
    "            return []\n",
    "    \n",
    "    def _local_paraphrase(self, question: str, n: int) -> List[str]:\n",
    "        \"\"\"Basit kural tabanlÄ± paraphrase\"\"\"\n",
    "        synonyms = {\n",
    "            'nasÄ±l': ['ne ÅŸekilde', 'hangi yolla', 'ne biÃ§imde'],\n",
    "            'nedir': ['ne demektir', 'nedir', 'ne anlama gelir'],\n",
    "            'yapabilirim': ['yapabilir miyim', 'yapmak mÃ¼mkÃ¼n mÃ¼', 'yapma imkanÄ± var mÄ±'],\n",
    "            'Ã¶ÄŸrenmek': ['bilgi edinmek', 'hakkÄ±nda bilgi almak', 'Ã¶ÄŸrenmek'],\n",
    "            'ne zaman': ['hangi zamanda', 'ne vakit'],\n",
    "            'nerede': ['hangi yerde', 'nerede'],\n",
    "            'kim': ['hangi kiÅŸi', 'kimler'],\n",
    "        }\n",
    "        \n",
    "        paraphrases = []\n",
    "        \n",
    "        for i in range(min(n, 10)):\n",
    "            new_q = question\n",
    "            for word, syns in synonyms.items():\n",
    "                if word in new_q.lower():\n",
    "                    new_q = new_q.lower().replace(word, np.random.choice(syns))\n",
    "            \n",
    "            # Varyasyonlar\n",
    "            if i % 4 == 0:\n",
    "                new_q = new_q.capitalize() + \"?\"\n",
    "            elif i % 4 == 1:\n",
    "                new_q = new_q.capitalize() + \" hakkÄ±nda bilgi verir misiniz?\"\n",
    "            elif i % 4 == 2:\n",
    "                new_q = new_q.capitalize() + \" konusunda yardÄ±mcÄ± olabilir misiniz?\"\n",
    "            else:\n",
    "                new_q = new_q.capitalize()\n",
    "            \n",
    "            if new_q not in paraphrases:\n",
    "                paraphrases.append(new_q)\n",
    "        \n",
    "        return paraphrases[:n]\n",
    "    \n",
    "    def _anthropic_paraphrase(self, question: str, n: int) -> List[str]:\n",
    "        \"\"\"Claude API ile paraphrase\"\"\"\n",
    "        url = \"https://api.anthropic.com/v1/messages\"\n",
    "        headers = {\n",
    "            \"x-api-key\": self.api_key,\n",
    "            \"anthropic-version\": \"2023-06-01\",\n",
    "            \"content-type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        paraphrases = []\n",
    "        batch_size = 5\n",
    "        \n",
    "        for batch_idx in range(0, n, batch_size):\n",
    "            batch_n = min(batch_size, n - batch_idx)\n",
    "            \n",
    "            prompt = f\"\"\"AÅŸaÄŸÄ±daki soruyu {batch_n} farklÄ± ÅŸekilde yeniden ifade et.\n",
    "\n",
    "KURALLAR:\n",
    "- Sorunun ANLAMINI ve AMACINI KESINLIKLE deÄŸiÅŸtirme\n",
    "- Yeni bilgi EKLEME veya Ã‡IKARMA\n",
    "- Her varyasyon farklÄ± bir tarzda olsun\n",
    "- Her satÄ±ra sadece BIR soru yaz\n",
    "- Ekstra aÃ§Ä±klama yapma\n",
    "\n",
    "SORU: {question}\n",
    "\n",
    "Ã‡IKTI:\"\"\"\n",
    "\n",
    "            payload = {\n",
    "                \"model\": \"claude-3-5-sonnet-20241022\",\n",
    "                \"max_tokens\": 1024,\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                response = requests.post(url, headers=headers, json=payload, timeout=30)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    content = response.json()['content'][0]['text']\n",
    "                    lines = [l.strip() for l in content.strip().split('\\n') if l.strip() and '?' in l]\n",
    "                    paraphrases.extend(lines[:batch_n])\n",
    "                elif response.status_code == 429:\n",
    "                    print(\"â³ Rate limit, bekleniyor...\")\n",
    "                    time.sleep(10)\n",
    "                \n",
    "                time.sleep(1)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Request hatasÄ±: {e}\")\n",
    "        \n",
    "        return paraphrases[:n]\n",
    "    \n",
    "    def _openai_paraphrase(self, question: str, n: int) -> List[str]:\n",
    "        \"\"\"OpenAI API ile paraphrase\"\"\"\n",
    "        # Benzer implementasyon\n",
    "        return self._local_paraphrase(question, n)\n",
    "\n",
    "def generate_paraphrases(df: pd.DataFrame, generator: ParaphraseGenerator, \n",
    "                        n_per_question: int, embeddings: np.ndarray, \n",
    "                        model: SentenceTransformer) -> pd.DataFrame:\n",
    "    \"\"\"TÃ¼m canonical sorular iÃ§in paraphrase Ã¼ret\"\"\"\n",
    "    print(f\"\\nğŸ­ Paraphrase Ã¼retimi ({n_per_question} varyasyon/soru)...\")\n",
    "    \n",
    "    canonical_questions = df[['cluster_id', 'canonical_question', 'canonical_answer']].drop_duplicates('cluster_id')\n",
    "    \n",
    "    all_rows = []\n",
    "    \n",
    "    for idx, row in tqdm(canonical_questions.iterrows(), total=len(canonical_questions), desc=\"Paraphrasing\", ncols=80):\n",
    "        question = row['canonical_question']\n",
    "        answer = row['canonical_answer']\n",
    "        cluster_id = row['cluster_id']\n",
    "        \n",
    "        paraphrases = generator.generate(question, n_per_question)\n",
    "        \n",
    "        if not paraphrases:\n",
    "            continue\n",
    "        \n",
    "        para_embeddings = model.encode(paraphrases, show_progress_bar=False)\n",
    "        orig_embedding = embeddings[df[df['cluster_id'] == cluster_id].index[0]].reshape(1, -1)\n",
    "        \n",
    "        for para, para_emb in zip(paraphrases, para_embeddings):\n",
    "            similarity = cosine_similarity(orig_embedding, para_emb.reshape(1, -1))[0][0]\n",
    "            \n",
    "            if similarity > 0.95 or similarity < 0.70:\n",
    "                continue\n",
    "            \n",
    "            all_rows.append({\n",
    "                'question': para,\n",
    "                'answer': answer,\n",
    "                'canonical_question': question,\n",
    "                'canonical_id': cluster_id,\n",
    "                'is_canonical': False,\n",
    "                'augmentation_method': f'paraphrase_{generator.provider}',\n",
    "                'confidence': float(similarity),\n",
    "                'source': 'generated'\n",
    "            })\n",
    "    \n",
    "    print(f\"âœ… {len(all_rows)} yeni paraphrase oluÅŸturuldu\")\n",
    "    \n",
    "    return pd.DataFrame(all_rows)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6. Metadata ZenginleÅŸtirme\n",
    "\n",
    "# %%\n",
    "def enrich_metadata(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Metadata zenginleÅŸtir\"\"\"\n",
    "    print(\"\\nğŸ·ï¸  Metadata zenginleÅŸtiriliyor...\")\n",
    "    \n",
    "    df['id'] = df.apply(lambda x: hashlib.md5(f\"{x['question']}_{x['answer']}\".encode()).hexdigest()[:16], axis=1)\n",
    "    \n",
    "    if 'canonical_id' not in df.columns:\n",
    "        df['canonical_id'] = df.get('cluster_id', 0)\n",
    "    \n",
    "    if 'is_canonical' not in df.columns:\n",
    "        df['is_canonical'] = False\n",
    "    \n",
    "    # Kategori zaten var, eksikse doldur\n",
    "    if 'category' not in df.columns or df['category'].isna().any():\n",
    "        def predict_category(question: str) -> str:\n",
    "            question_lower = question.lower()\n",
    "            \n",
    "            if any(kw in question_lower for kw in ['kayÄ±t', 'Ã¼ye', 'hesap']):\n",
    "                return 'account'\n",
    "            elif any(kw in question_lower for kw in ['ders', 'kurs', 'eÄŸitim', 'bootcamp']):\n",
    "                return 'courses'\n",
    "            elif any(kw in question_lower for kw in ['Ã¶deme', 'fiyat', 'Ã¼cret']):\n",
    "                return 'payment'\n",
    "            elif any(kw in question_lower for kw in ['sertifika', 'diploma']):\n",
    "                return 'certification'\n",
    "            elif any(kw in question_lower for kw in ['destek', 'yardÄ±m', 'sorun']):\n",
    "                return 'support'\n",
    "            else:\n",
    "                return 'general'\n",
    "        \n",
    "        df['category'] = df['category'].fillna(df['question'].apply(predict_category))\n",
    "    \n",
    "    def predict_intent(question: str) -> str:\n",
    "        question_lower = question.lower()\n",
    "        \n",
    "        if any(kw in question_lower for kw in ['nasÄ±l', 'ne ÅŸekilde', 'how']):\n",
    "            return 'how_to'\n",
    "        elif any(kw in question_lower for kw in ['nedir', 'ne demek', 'what']):\n",
    "            return 'definition'\n",
    "        elif any(kw in question_lower for kw in ['neden', 'niÃ§in', 'why']):\n",
    "            return 'reasoning'\n",
    "        elif any(kw in question_lower for kw in ['ne zaman', 'when']):\n",
    "            return 'timing'\n",
    "        else:\n",
    "            return 'information'\n",
    "    \n",
    "    df['intent'] = df['question'].apply(predict_intent)\n",
    "    \n",
    "    # Keywords zaten var, eksikse doldur\n",
    "    if 'keywords' not in df.columns or df['keywords'].isna().any():\n",
    "        def extract_keywords(text: str) -> str:\n",
    "            words = re.findall(r'\\b\\w{4,}\\b', text.lower())\n",
    "            return ', '.join(list(set(words))[:5])\n",
    "        \n",
    "        df['keywords'] = df['keywords'].fillna(df['question'].apply(extract_keywords))\n",
    "    \n",
    "    # Keywords listeyse stringe Ã§evir\n",
    "    df['keywords'] = df['keywords'].apply(lambda x: ', '.join(x) if isinstance(x, list) else str(x))\n",
    "    \n",
    "    df['entities'] = ''\n",
    "    \n",
    "    # Difficulty zaten var, eksikse hesapla\n",
    "    if 'difficulty_level' in df.columns:\n",
    "        df['difficulty'] = df['difficulty_level']\n",
    "    else:\n",
    "        def estimate_difficulty(answer: str) -> str:\n",
    "            length = len(answer)\n",
    "            if length < 100:\n",
    "                return 'easy'\n",
    "            elif length < 300:\n",
    "                return 'medium'\n",
    "            else:\n",
    "                return 'hard'\n",
    "        \n",
    "        df['difficulty'] = df['answer'].apply(estimate_difficulty)\n",
    "    \n",
    "    if 'augmentation_method' not in df.columns:\n",
    "        df['augmentation_method'] = 'original'\n",
    "    \n",
    "    if 'confidence' not in df.columns:\n",
    "        df['confidence'] = 1.0\n",
    "    \n",
    "    if 'created_at' not in df.columns:\n",
    "        df['created_at'] = datetime.now().isoformat()\n",
    "    \n",
    "    print(\"âœ… Metadata zenginleÅŸtirme tamamlandÄ±\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7. Ana Ä°ÅŸlem\n",
    "\n",
    "# %%\n",
    "# Logger kur\n",
    "log_dir = os.path.join(CONFIG['out_dir'], 'logs')\n",
    "logger = setup_logger(log_dir)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸš€ DEEP SSS ENRICHER BAÅLATILDI\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# %% \n",
    "# 1. Veri YÃ¼kleme\n",
    "df1 = load_dataset(CONFIG['input1'])\n",
    "df2 = load_dataset(CONFIG['input2'])\n",
    "\n",
    "# %%\n",
    "# 2. BirleÅŸtirme ve Temizleme\n",
    "df = merge_and_deduplicate(df1, df2)\n",
    "\n",
    "# %%\n",
    "# 3. Embedding OluÅŸturma\n",
    "embeddings, model = create_embeddings(df['question'].tolist())\n",
    "\n",
    "# %%\n",
    "# 4. Semantik KÃ¼meleme\n",
    "df = cluster_questions(df, embeddings, CONFIG['cluster_threshold'])\n",
    "\n",
    "# %%\n",
    "# 5. Canonical Sorular\n",
    "df, review_df = canonicalize_clusters(df)\n",
    "\n",
    "canonical_mask = df.groupby('cluster_id')['question'].transform(lambda x: x == x.iloc[0])\n",
    "df['is_canonical'] = canonical_mask\n",
    "\n",
    "# %%\n",
    "# 6. Paraphrase Ãœretimi\n",
    "if CONFIG['n_paraphrases'] > 0:\n",
    "    api_key = CONFIG['api_key'] or os.getenv('ANTHROPIC_API_KEY') or os.getenv('OPENAI_API_KEY')\n",
    "    \n",
    "    if CONFIG['provider'] != 'local' and not api_key:\n",
    "        print(\"âš ï¸  API key bulunamadÄ±, 'local' moda geÃ§iliyor\")\n",
    "        CONFIG['provider'] = 'local'\n",
    "    \n",
    "    generator = ParaphraseGenerator(CONFIG['provider'], api_key, CONFIG['seed'])\n",
    "    para_df = generate_paraphrases(df, generator, CONFIG['n_paraphrases'], embeddings, model)\n",
    "    \n",
    "    df = pd.concat([df, para_df], ignore_index=True)\n",
    "    print(f\"\\nğŸ“Š Toplam satÄ±r (paraphrase sonrasÄ±): {len(df)}\")\n",
    "\n",
    "# %%\n",
    "# 7. Metadata ZenginleÅŸtirme\n",
    "df = enrich_metadata(df)\n",
    "\n",
    "# %%\n",
    "# 8. SonuÃ§larÄ± Kaydetme\n",
    "os.makedirs(CONFIG['out_dir'], exist_ok=True)\n",
    "\n",
    "df.to_json(os.path.join(CONFIG['out_dir'], 'enriched_dataset.jsonl'), \n",
    "           orient='records', lines=True, force_ascii=False)\n",
    "df.to_csv(os.path.join(CONFIG['out_dir'], 'enriched_dataset.csv'), \n",
    "          index=False, encoding='utf-8')\n",
    "\n",
    "canonical_df = df[df['is_canonical'] == True][['cluster_id', 'canonical_question', \n",
    "                                                 'canonical_answer', 'category', 'keywords']]\n",
    "canonical_df.to_csv(os.path.join(CONFIG['out_dir'], 'canonical_questions.csv'), \n",
    "                    index=False, encoding='utf-8')\n",
    "\n",
    "if len(review_df) > 0:\n",
    "    review_df.to_csv(os.path.join(CONFIG['out_dir'], 'review_needed.csv'), \n",
    "                     index=False, encoding='utf-8')\n",
    "\n",
    "df.head(50).to_csv(os.path.join(CONFIG['out_dir'], 'preview.csv'), \n",
    "                   index=False, encoding='utf-8')\n",
    "\n",
    "# %%\n",
    "# 9. Manifest OluÅŸturma\n",
    "manifest = {\n",
    "    'creation_date': datetime.now().isoformat(),\n",
    "    'total_records': int(len(df)),\n",
    "    'original_records': int(len(df[df['augmentation_method'] == 'original'])),\n",
    "    'paraphrased_records': int(len(df[df['augmentation_method'].str.contains('paraphrase', na=False)])),\n",
    "    'n_clusters': int(df['cluster_id'].nunique()),\n",
    "    'n_canonical': int(df['is_canonical'].sum()),\n",
    "    'provider': CONFIG['provider'],\n",
    "    'n_paraphrases_per_question': CONFIG['n_paraphrases'],\n",
    "    'seed': CONFIG['seed'],\n",
    "    'categories': {str(k): int(v) for k, v in df['category'].value_counts().to_dict().items()},\n",
    "    'intents': {str(k): int(v) for k, v in df['intent'].value_counts().to_dict().items()},\n",
    "    'augmentation_methods': {str(k): int(v) for k, v in df['augmentation_method'].value_counts().to_dict().items()},\n",
    "    'difficulty_distribution': {str(k): int(v) for k, v in df['difficulty'].value_counts().to_dict().items()},\n",
    "    'review_needed_count': int(len(review_df)),\n",
    "    'processing_time_seconds': round(time.time() - start_time, 2)\n",
    "}\n",
    "\n",
    "with open(os.path.join(CONFIG['out_dir'], 'manifest.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(manifest, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"\\nğŸ’¾ TÃ¼m Ã§Ä±ktÄ±lar kaydedildi!\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 8. SonuÃ§ Ã–zeti\n",
    "\n",
    "# %%\n",
    "print(\"=\"*80)\n",
    "print(\"âœ… Ä°ÅLEM TAMAMLANDI\")\n",
    "print(\"=\"*80)\n",
    "print(f\"ğŸ“Š Toplam kayÄ±t: {manifest['total_records']:,}\")\n",
    "print(f\"ğŸ“ Orijinal kayÄ±t: {manifest['original_records']:,}\")\n",
    "print(f\"ğŸ­ Paraphrase kayÄ±t: {manifest['paraphrased_records']:,}\")\n",
    "print(f\"ğŸ” KÃ¼me sayÄ±sÄ±: {manifest['n_clusters']:,}\")\n",
    "print(f\"â­ Canonical soru: {manifest['n_canonical']:,}\")\n",
    "print(f\"âš ï¸  Ä°nceleme gereken: {manifest['review_needed_count']:,}\")\n",
    "print(f\"â±ï¸  Ä°ÅŸlem sÃ¼resi: {manifest['processing_time_seconds']:.2f} saniye\")\n",
    "print(f\"ğŸ“ Ã‡Ä±ktÄ± dizini: {CONFIG['out_dir']}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# %%\n",
    "# Ã–rnek sonuÃ§larÄ± gÃ¶rÃ¼ntÃ¼le\n",
    "print(\"\\nğŸ“‹ Ã–rnek Canonical Sorular:\")\n",
    "display(canonical_df.head(10))\n",
    "\n",
    "print(\"\\nğŸ“‹ Ã–rnek Paraphrase'ler:\")\n",
    "display(df[df['augmentation_method'].str.contains('paraphrase', na=False)].head(10))\n",
    "\n",
    "print(\"\\nğŸ“Š Kategori DaÄŸÄ±lÄ±mÄ±:\")\n",
    "display(df['category'].value_counts())\n",
    "\n",
    "print(\"\\nğŸ“Š Intent DaÄŸÄ±lÄ±mÄ±:\")\n",
    "display(df['intent'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9ff417-6ba0-488f-a37f-7bab981eff15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
